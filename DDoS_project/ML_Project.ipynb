{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxwCGEFXMG3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af6c302-02f7-4de5-984c-4dd7708c1c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (85) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(191694, 88)\n",
            "(57508, 87)\n"
          ]
        }
      ],
      "source": [
        "# CS519/487(Spring) Project\n",
        "# DDoS attack Detection techniques\n",
        "# Name: Shengping Bi, Jingru Dou, Yuxi Wang\n",
        "# Date: 03/13/2022\n",
        "# Purpose: This Project is for detecting Distributed Denial of Service (DDoS) Cyberattacks efficiently using (AI) and ML algorithms.\n",
        "\n",
        "#****************************************************************************************************\n",
        "# Start to include library or python package\n",
        "#****************************************************************************************************\n",
        "# system gerneral library\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "#****************************************************************************************************\n",
        "# all the lib with sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier  # import decision tree\n",
        "from sklearn.preprocessing import StandardScaler  # Features scaling\n",
        "from sklearn.metrics import accuracy_score  # find the accuracy of classifier\n",
        "\n",
        "#****************************************************************************************************\n",
        "# Start load data file from google drive\n",
        "df_portmap = pd.read_csv('/content/Portmap.csv')\n",
        "#****************************************************************************************************\n",
        "#****************************************************************************************************\n",
        "print(df_portmap.shape)\n",
        "\n",
        "data_df = df_portmap.dropna(axis=0)# drop the empty col and row\n",
        "data_X = data_df.drop([' Label'], axis=1)# drop the col with label\n",
        "data_y = data_df[' Label']# the label of data set will be the target\n",
        "\n",
        "# in order to reduce the running time, call the train_test_split to get the subset of initial dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.3, random_state=42)\n",
        "print(X_test.shape) \n",
        "\n",
        "data_subset = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "data_subset.to_csv('Portmap_subset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#****************************************************************************************************\n",
        "# Start to  Dimensionality reduction\n",
        "#****************************************************************************************************\n",
        "# system gerneral library\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "# Load packet for Dimensionality reduction techniques\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler  # Features scaling\n",
        "\n",
        "#****************************************************************************************************\n",
        "# Start load data file from google drive\n",
        "#****************************************************************************************************\n",
        "df_portmap_subset = pd.read_csv('/content/Portmap_subset.csv')\n",
        "\n",
        "X = data_df.drop([' Label'], axis=1)# drop the col with label\n",
        "y = data_df[' Label']# the label of data set will be the target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# do the data standarlize\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "#****************************************************************************************************\n",
        "# Do the PCA Dimensionality reduction techniques\n",
        "#****************************************************************************************************\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n"
      ],
      "metadata": {
        "id": "F6Jws7tbWJY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}